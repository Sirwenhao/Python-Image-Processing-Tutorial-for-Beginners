#### 《Python图像处理实战》Chapter07学习笔记

图像的特征提取与描述符

主要内容：

- 特征检测器与描述符
- 哈里斯角点检测器
- 基于LoG、DoG和DoH的斑点检测器
- 基于方向梯度直方图的特征提取
- 尺度不变性变换(SIFT)
- 类Haar特征及其在人脸检测中的应用

##### 检测器与描述符：

所谓的检测器是指从图像中选取某一组感兴趣特征所采用的标准(如角点、局部最大值、局部最小值等)；描述符指图像特征值或感兴趣点的集合。

关于图像角点定义的参考文献：

- 角点是其局部邻域位于两个主要且不同的边缘方向上的点，简单来说，角点可以解释为两个边缘的交界处
- https://www.cs.ubc.ca/~nickhar/W13/Lecture3.pdf
- https://blog.csdn.net/yizhang_ml/article/details/86994193

##### 哈里斯角点检测器：

Definition from Wikipedia：The **Harris corner detector** is a [corner detection](https://en.wikipedia.org/wiki/Corner_detection) operator that is commonly used in [computer vision](https://en.wikipedia.org/wiki/Computer_vision) algorithms to extract corners and infer [features](https://en.wikipedia.org/wiki/Feature_(computer_vision)) of an image. It was first introduced by Chris Harris and Mike Stephens in 1988 upon the improvement of [Moravec's corner detector](https://en.wikipedia.org/wiki/Corner_detection#Moravec_corner_detection_algorithm).[[1\]](https://en.wikipedia.org/wiki/Harris_corner_detector#cite_note-harris-1) Compared to the previous one, Harris' corner detector takes the differential of the corner score into account with reference to direction directly, instead of using shifting patches for every 45 degree angles, and has been proved to be more accurate in distinguishing between edges and corners.[[2\]](https://en.wikipedia.org/wiki/Harris_corner_detector#cite_note-dey-2) Since then, it has been improved and adopted in many algorithms to preprocess images for subsequent applications.

哈里斯角点检测器主要探究了当窗口在图像中改变位置时，窗口内强度的变化，且该算法中强度值在所有方向的角点处都有显著的变化。哈里斯角点检测算法对图像的旋转变化不敏感，但当图像大小调整时角点信息会发生改变。

对于角点更为精细的检测，可以使用scikit-image特征模块中的corner_subpix()函数，可以以子像素的准确率对检测到的角点进行细化。

应用一：哈里斯角点特征用于图像匹配

应用二：基于RANSAC(随机抽样一致性算法和哈里斯角点特征的鲁棒图像匹配

##### 基于LoG、DoG和DoH的斑点检测器

斑点被定义为黑暗区域上的亮斑或明亮区域上的暗斑，`skimage.feature`模块下的`blob_dog`、`blob_log`、`blog_doh`可用于对图像进行斑点检测。

黑塞矩阵定义：

![image-20221202203256875](https://gitee.com/sirwenhao/typora-illustration/raw/master/image-20221202203256875.png)

##### 方向梯度直方图(Histogram of Oriented Gradient)

是一种常用的目标检测特征描述符，HOG描述符的具体算法步骤如下：

1. 对图像进行全局归一化处理(可选择)
2. 计算水平和垂直梯度图像
3. 计算梯度直方图
4. 块(区域)集归一化处理
5. 扁平组合成特征描述向量

经过上述步骤，HOG最终得到的是归一化区间描述符。

- RIEF 特征进行去相关的学习方法，从而在最近邻应用程序中获得更好的性能

##### Introduction to SIFT

SIFT stands for Scale-Invariant Feature Transform and was first presented in 2004, by **D.Lowe**, University of British Columbia. SIFT is invariance to image scale and rotation. 

Major advantages of SIFT are:

- **Locality:** features are local, so robust to occlusion and clutter (no prior segmentation)
- **Distinctiveness:** individual features can be matched to a large database of objects
- **Quantity:** many features can be generated for even small objects
- **Efficiency:** close to real-time performance
- **Extensibility:** can easily be extended to a wide range of different feature types, with each adding robustness

There are mainly four steps involved in the SIFT algorithm:

- **Scale-space peak selection:** Potential location for finding features.
- **Keypoint Localization:** Accurately locating the feature keypoints.
- **Orientation Assignment:** Assigning orientation to keypoints.
- **Keypoint descriptor:** Describing the keypoints as a high dimensional vector.
- **Keypoint Matching**

**Step I : Scale-space peak Selection**

- Scale Space：

  The scale space of an image is a function L(x,y,σ) that is produced from the convolution of a Gaussian kernel(Blurring) at different scales with the input image.Scale-space is separated into octaves and the number of octaves and scale depends on the size of the original image. So we generate several octaves of the original image. Each octave’s image size is half the previous one.

![image-20221204161755821](https://gitee.com/sirwenhao/typora-illustration/raw/master/image-20221204161755821.png)

- Blurring：

  Within an octave, images are progressively blurred using the Gaussian Blur operator. Mathematically, “blurring” is referred to as the convolution of the Gaussian operator and the image. Gaussian blur has a particular expression or “operator” that is applied to each pixel. What results is the blurred image. Blurred image by Gaussian: $L(x,y,\sigma)=G(x,y,\sigma)*I(x,y)$. G is the Gaussian Blur operator and I is an image. While x,y are the location coordinates and $σ$ is the “scale” parameter. Think of it as the amount of blur. Greater the value, greater the blur. Gaussian Blur Operator: $G(x,y,\sigma)=\frac{1}{2\pi\sigma^2}e^{-(x^2+y^2)/2\sigma^2}$

- DOG(Difference of Gaussian kernel): 

  Now we use those blurred images to generate another set of images, the Difference of Gaussians (DoG). These DoG images are great for finding out interesting keypoints in the image. The difference of Gaussian is obtained as the difference of Gaussian blurring of an image with two different σ, let it be σ and *kσ*. This process is done for different octaves of the image in the Gaussian Pyramid. It is represented in below image:

  ![image-20221204163022909](https://gitee.com/sirwenhao/typora-illustration/raw/master/image-20221204163022909.png)

**Step II: Keypoint Localization**

- Finding keypoints:

  Up till now, we have generated a scale space and used the scale space to calculate the Difference of Gaussians. Those are then used to calculate Laplacian of Gaussian approximations that are scale invariant. One pixel in an image is compared with its 8 neighbors as well as 9 pixels in the next scale and 9 pixels in previous scales. This way, a total of 26 checks are made. If it is a local extrema, it is a potential keypoint. It basically means that keypoint is best represented in that scale.	

  ![image-20221204163412382](https://gitee.com/sirwenhao/typora-illustration/raw/master/image-20221204163412382.png)

- Keypoint Localization

  In the previous step, a lot of keypoints produced. Some of them lie along an edge, or they don’t have enough contrast. In both cases, they are not as useful as features. So we get rid of them. The approach is similar to the one used in the Harris Corner Detector for removing edge features. For low contrast features, we simply check their intensities. They used Taylor series expansion of scale space to get a more accurate location of extrema, and if the intensity at this extrema is less than a threshold value (0.03 as per the paper), it is rejected. DoG has a higher response for edges, so edges also need to be removed. They used a 2x2 Hessian matrix (H) to compute the principal curvature.

  ![image-20221204165314200](https://gitee.com/sirwenhao/typora-illustration/raw/master/image-20221204165314200.png)

**Step III: Orientation Assignment**

Now we have legitimate keypoints. They’ve been tested to be stable. We already know the scale at which the keypoint was detected (it’s the same as the scale of the blurred image). So we have scale invariance. The next thing is to assign an orientation to each keypoint to make it rotation invariance.